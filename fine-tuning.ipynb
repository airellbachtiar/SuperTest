{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine Tuning\n",
    "\n",
    "I was tasked to scratch the surface of fine tuning LLM for OpenAI and Anthropic. Unfortunately, Anthropic only provide fine tuning the model on [Amazon Bedrock](https://aws.amazon.com/bedrock/claude/), it is an external service that is in collaboration with Anthropic. However, OpenAI provide the ability for their user to train their model, albeit only their selected models are available to be fine tuned and it will cost more than regular API call. This cost addition includes training time and hosting the trained model to be used. Moreover, not everything requires the model to be fine tuned, OpenAI's GPT-4o is powerful enough to handle the majority of the tasks given to it and usually to achieve better performance, an effective prompt engineering is enough.\n",
    "\n",
    "### What is it for?\n",
    "1. The model can perform a better response for a task\n",
    "2. It will train on the example as a train data so that it can be omitted in the prompt leading to saving tokens\n",
    "3. Faster response time\n",
    "\n",
    "### When to use it?\n",
    "If and only if prompt engineering and prompt chaining strategies does not meet the expected outcome from an LLM. [Function calling](https://platform.openai.com/docs/guides/function-calling) is another method OpenAI recommended. Fine tuning requires a careful investment of time and effort. It is faster to the recommended method before diving into fine tuning since it is much more complicated. And it is better to start with the recommended strategies since it will also be used for the fine tuned model.\n",
    "\n",
    "### Common use case:\n",
    "1. Setting the style, tone, format, or other qualitative aspects\n",
    "2. Improving reliability at producing a desired output\n",
    "3. Correcting failures to follow complex prompts\n",
    "4. Handling many edge cases in specific ways\n",
    "5. Performing a new skill or task that’s hard to articulate in a prompt\n",
    "\n",
    "### How to do it?\n",
    "At high level, these are the steps to fine tune a model:\n",
    "1. Prepare and upload training data\n",
    "2. Train a model or fine tuned model using it\n",
    "3. Evaluate result and go back to step 1 if needed\n",
    "4. Use the fine tuned model\n",
    "\n",
    "### Available fine tuned model\n",
    "Fine-tuning is currently available for the following models:\n",
    "\n",
    "1.      gpt-4o-2024-08-06\n",
    "2.      gpt-4o-mini-2024-07-18\n",
    "3.      gpt-4-0613\n",
    "4.      gpt-3.5-turbo-0125\n",
    "5.      gpt-3.5-turbo-1106\n",
    "6.      gpt-3.5-turbo-0613\n",
    "\n",
    "## Tutorial\n",
    "### 1. Prepare Dataset\n",
    "The format of the data is the same format as the request body when you wish to use the [chat completions API](https://platform.openai.com/docs/api-reference/chat/create). It can contain a single assistant role or multiple assistants in a single conversation data. `weight` can also be added to indicate that the response is used as data (1) or ignores the response (0). Here is a format example of what dataset to train an LLM looks like:\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\", \"weight\": 1}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"William Shakespeare\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\", \"weight\": 1}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"384,400 kilometers\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\", \"weight\": 1}]}\n",
    "```\n",
    "\n",
    "In order to have successful fine tuning, the dataset should at least contain 10 examples. However, 50 to 100 training examples are recommended to see clear improvements.\n",
    "\n",
    "### 2. Train and Test\n",
    "It is a common practice when training AI to split the dataset to train and test set. Usually it is split 80/20. When splitting, don't forget to randomize the dataset first to achieve balance and un-biased train and test dataset. Test dataset is useful for evaluation of the fine tuned LLM.\n",
    "\n",
    "### 3. Use fine tuned model\n",
    "The dataset file should be in `.jsonl` and to upload this file for training, it uses [Files API](https://platform.openai.com/docs/api-reference/files/create) and here is the example of creating a fine tuning job with DPO (Direct Preference Optimization):\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-all-about-the-weather\",\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    method={\n",
    "        \"type\": \"dpo\",\n",
    "        \"dpo\": {\n",
    "            \"hyperparameters\": {\"beta\": 0.1},\n",
    "        },\n",
    "    },\n",
    ")\n",
    "```\n",
    "or\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-abc123\",\n",
    "    model=\"gpt-4o-mini-2024-07-18\"\n",
    ")\n",
    "```\n",
    "\n",
    "or optionally, you can create the job via [fine-tuning UI](https://platform.openai.com/finetune).\n",
    "\n",
    "When a method is not specified, the default method is Supervised Fine-Tuning (SFT). For more information please refer to [this](https://platform.openai.com/docs/api-reference/fine-tuning/create) API documentation.\n",
    "\n",
    "After started the fine tuning job, you can list existing jobs, retrieve the status of a job, or cancel a job. Though it seems that the example provided by OpenAI uses their library.\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(\"ftjob-abc123\")\n",
    "\n",
    "# Cancel a job\n",
    "client.fine_tuning.jobs.cancel(\"ftjob-abc123\")\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-abc123\", limit=10)\n",
    "\n",
    "# Delete a fine-tuned model\n",
    "client.models.delete(\"ft:gpt-3.5-turbo:acemeco:suffix:abc123\")\n",
    "```\n",
    "\n",
    "During and after training the model, OpenAI provide metrics to be monitor which are:\n",
    "1. training loss\n",
    "2. training token accuracy\n",
    "3. valid loss\n",
    "4. valid token accuracy\n",
    "We want to achieve a decrease of loss and increase on accuracy for a successful training.\n",
    "\n",
    "### 4. Evaluate\n",
    "https://platform.openai.com/docs/guides/evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling\n",
    "\n",
    "Function calling is a type of tools that can be sent to OpenAI API alongside the message you usually send through the API. This tool will let the LLM know that there is a function exist within our system that we created and can be used for extra information that the LLM needed. It is considered as an extra context for the LLM to consider whether we have to use the given function for more information the LLM needs. The LLM may choose to ask us (developer/user) to execute this function and send the result through the API.\n",
    "\n",
    "For example, I created a weather app which has a method called ```get_weather```. I will ask an unrelated question first to the LLM like \"What is the capital city of Spain?\" whilst providing this extra information called function calling in the API request body. The request body would roughly looks like the following:\n",
    "```json\n",
    "\"model\": \"gpt-4o\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital city of Spain?\"\n",
    "        }\n",
    "    ],\n",
    "\"tools\": [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current temperature for a given location.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\n",
    "                        \"location\"\n",
    "                    ],\n",
    "                    \"additionalProperties\": false\n",
    "                },\n",
    "             \"strict\": true\n",
    "        }\n",
    "    }\n",
    "]\n",
    "```\n",
    "This first message is does not requires the function calling tool, so the LLM will \"ignore\" this function calling and answer thew question normally. But, if the next question asks the question \"How's the weather in there?\" referring to the answer the LLM gave provided that the conversation between us and the LLM are included in the messages property and function calling tool still included in the call, the LLM will ask us to run the function that we created by returning this:\n",
    "```json\n",
    "{ \"role\": \"assistant\", \"content\": null, \"function_call\": { \"name\": \"get_weather\", \"arguments\": \"{ \\\"location\\\": \\\"Madrid\\\" }\" } }\n",
    "\n",
    "```\n",
    "To run this function, it can be either we as developer setup a automated execution of this function or execute it manually depends on the preference. But the important part is that the result should be included in the next call while also providing the conversation history between us and it. And after we gave all of this conversation, the LLM can answers the question we asked about the weather with the help of the function we created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "1. https://platform.openai.com/docs/guides/fine-tuning\n",
    "2. https://aws.amazon.com/bedrock/claude/\n",
    "3. https://www.anthropic.com/news/fine-tune-claude-3-haiku\n",
    "4. https://platform.openai.com/docs/guides/function-calling?example=get-weather\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "First option is to everything via Podman, create persistent volume -> create image -> export the model.\n",
    "Or, easier option, use Ollama to export the model itself.\n",
    "\n",
    "However, I will be explaining how to do it via Podman.\n",
    "First, pull ollama/ollama image from https://hub.docker.com/r/ollama/ollama.\n",
    "Execute the command bellow to create the persistent volume. Change the name of the model you wish to download (in this case, it is deepseek-r1:1.5b).\n",
    "\n",
    "```bash\n",
    "podman run --rm -v ollama-models:/root/.ollama --entrypoint sh ollama/ollama -c \"ollama serve & sleep 3 && ollama pull deepseek-r1:1.5b\"\n",
    "```\n",
    "Then create a temporary container to access model file.\n",
    "```bash\n",
    "podman create --name ollama-temp -v ollama-models:/root/.ollama ollama/ollama\n",
    "```\n",
    "Then copy the downloaded model in Podman to your local machine. Make sure that you create a folder named ollama-export in your root folder.\n",
    "```bash\n",
    "podman cp ollama-temp:/root/.ollama ./ollama-export\n",
    "```\n",
    "And finally, remove the temporary container.\n",
    "```bash\n",
    "podman rm ollama-temp\n",
    "```\n",
    "\n",
    "Then in another folder, first create a container file or docker file like this:\n",
    "```Dockerfile\n",
    "# Use Ollama as the base image\n",
    "FROM ollama/ollama\n",
    "\n",
    "# Copy the pre-downloaded model files into the image\n",
    "COPY ollama-export /root/.ollama\n",
    "\n",
    "# Ensure correct permissions\n",
    "RUN chmod -R 777 /root/.ollama\n",
    "\n",
    "# Start the Ollama server and run the model\n",
    "COPY entrypoint.sh /entrypoint.sh\n",
    "RUN chmod +x /entrypoint.sh && sed -i 's/\\r$//' /entrypoint.sh\n",
    "\n",
    "ENTRYPOINT [\"/entrypoint.sh\"]\n",
    "```\n",
    "\n",
    "Create a `.sh` file to run the model.\n",
    "```sh\n",
    "#!/bin/bash\n",
    "# Start the Ollama server in the background\n",
    "ollama serve &\n",
    "\n",
    "# Wait for the server to initialize\n",
    "sleep 3\n",
    "\n",
    "# Run the model interactively\n",
    "exec ollama run deepseek-r1:1.5b\n",
    "```\n",
    "\n",
    "And finally, copy or move the `ollama-export` folder to the same folder as the container file."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
